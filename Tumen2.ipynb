{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgM-ePazILXL"
      },
      "outputs": [],
      "source": [
        "!wget https://lodmedia.hb.bizmrg.com/case_files/802656/train_dataset_train.zip\n",
        "!unzip ./train_dataset_train.zip\n",
        "!pip install pymorphy2\n",
        "!pip install implicit -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import dbscan\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier, RidgeClassifier, LogisticRegressionCV,Ridge,QuantileRegressor,PassiveAggressiveClassifier\n",
        "from sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier,RandomForestClassifier,VotingClassifier,RandomForestRegressor,GradientBoostingClassifier,StackingRegressor,BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC,LinearSVR,SVR\n",
        "from sklearn.decomposition import TruncatedSVD,PCA,FactorAnalysis,IncrementalPCA,FastICA,KernelPCA,NMF,LatentDirichletAllocation\n",
        "from sklearn.preprocessing import RobustScaler,QuantileTransformer,PowerTransformer,PolynomialFeatures,KBinsDiscretizer,StandardScaler,OneHotEncoder,OrdinalEncoder,FunctionTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline,FeatureUnion,TransformerMixin\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor,LocalOutlierFactor\n",
        "from sklearn.model_selection import train_test_split,ShuffleSplit,StratifiedShuffleSplit,TimeSeriesSplit,GridSearchCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import KNNImputer,SimpleImputer\n",
        "from sklearn.dummy import DummyRegressor,DummyClassifier\n",
        "from sklearn import set_config\n",
        "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error,roc_auc_score,accuracy_score,f1_score,classification_report\n",
        "from scipy.sparse import hstack,vstack\n",
        "import pymorphy2\n",
        "from implicit.als import AlternatingLeastSquares"
      ],
      "metadata": {
        "id": "rlNzSD7iIbJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# считывание данных\n",
        "users = pd.read_csv('./train/users.csv', sep=';', index_col=None, dtype={'age': str, 'chb': str, 'chit_type': str, 'gender': str})\n",
        "items = pd.read_csv('../input/tumen1/items.csv', sep=';', index_col=None, dtype={'author': str, 'bbk': str, 'izd': str, 'sys_numb': str, 'title': str, 'year_izd': str})\n",
        "train_transactions = pd.read_csv('./train/train_transactions_extended.csv', sep=';', index_col=None, dtype={'chb': str, 'date_1': str, 'is_printed': str, 'is_real': str, 'source': str, 'sys_numb': str, 'type': str})"
      ],
      "metadata": {
        "id": "gDyQkn3hIbGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# предобработка и лемматизация\n",
        "items['bbk']=items['bbk'].astype('str').str.replace('\\n',' ')\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "def lemmatize(text): \n",
        "    words = text.split()\n",
        "    res = list()\n",
        "    for word in words:\n",
        "        p = morph.parse(word)[0]\n",
        "        res.append(p.normal_form)\n",
        "\n",
        "    return ' '.join(res)\n",
        "items['title_l']=list(map(lemmatize, items['title'].tolist()))"
      ],
      "metadata": {
        "id": "x6c3BRZAIbDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# экстракция свойств из данных книжных изданий\n",
        "sparse_title_l=csr_matrix(CountVectorizer(max_df=0.1,min_df=4).fit_transform(items.title_l.astype('str')).T,shape=(38652,354355))\n",
        "sparse_author=csr_matrix(CountVectorizer(max_df=0.1,min_df=4).fit_transform(items.author).T,shape=(19921,354355))\n",
        "sparse_izd=csr_matrix(CountVectorizer(max_df=0.1,min_df=4).fit_transform(items.izd).T,shape=(7034,354355))\n",
        "sparse_bbk=csr_matrix(CountVectorizer(lowercase=False,token_pattern='[\\w|=|-]+', max_df=0.1,min_df=4).fit_transform(items.bbk.astype('str')).T,shape=(12826,354355))\n",
        "sparse_year_izd=csr_matrix(CountVectorizer(max_df=0.1,min_df=4).fit_transform(items.year_izd.astype('str')).T,shape=(384,354355))"
      ],
      "metadata": {
        "id": "H4R5Ihl-Ia_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# определение числа уникальных читателей, создание прямых и обратных словарей chb/sys_numb\n",
        "n_users = len(train_transactions['chb'].unique())\n",
        "\n",
        "mapping_chb_index = {chb_number: index for index, chb_number in enumerate(train_transactions['chb'].unique())}\n",
        "mapping_sys_numb_index = {sys_number: index for index, sys_number in enumerate(items['sys_numb'].unique())}\n",
        "\n",
        "mapping_index_chb = {index: chb_number for index, chb_number in enumerate(train_transactions['chb'].unique())}\n",
        "mapping_index_sys_numb = {index: sys_number for index, sys_number in enumerate(items['sys_numb'].unique())}"
      ],
      "metadata": {
        "id": "rTaRmuhDIaf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# делим данные на тренировочный и тестовый наборы\n",
        "train_transactions = train_transactions[['chb', 'sys_numb']]\n",
        "train_data, test_data = train_test_split(train_transactions, test_size=0.20,random_state=0)"
      ],
      "metadata": {
        "id": "s3bB1bZMIa8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# получение расширенной sparse матрицы user-item\n",
        "def df_to_sparse(df,users):\n",
        "    row = []\n",
        "    col = []\n",
        "    data = []\n",
        "\n",
        "    for line in df.itertuples():\n",
        "        row.append(mapping_chb_index[line.chb])\n",
        "        col.append(mapping_sys_numb_index[line.sys_numb])\n",
        "        data.append(1)\n",
        "    return csr_matrix((data, (row, col)))\n",
        "\n",
        "train_data_sparse = df_to_sparse(train_data,users)\n",
        "test_data_sparse = df_to_sparse(test_data,users)\n",
        "\n",
        "train_data_sparse=vstack((train_data_sparse,sparse_title_l,sparse_author,sparse_izd,sparse_bbk,sparse_yd))"
      ],
      "metadata": {
        "id": "1fBHKtX4IadR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# обучение модели\n",
        "model = AlternatingLeastSquares(factors=1000,regularization = 2.0,iterations=5)\n",
        "model.fit(train_data_sparse)"
      ],
      "metadata": {
        "id": "dshTOtvwIaaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#подбор рекомендаций для всех пользователей\n",
        "def get_recom(userid):\n",
        "    ids, scores = model.recommend(userid, train_data_sparse[userid], N=20, filter_already_liked_items=True)\n",
        "    top20recom_df = pd.DataFrame({\"sys_numb\": [mapping_index_sys_numb[id] for id in ids], \"score\": scores, \"already_liked\": np.in1d(ids, train_data_sparse[userid].indices)})\n",
        "    return top20recom_df['sys_numb'].values\n",
        "\n",
        "all_rec = []\n",
        "\n",
        "for userid in tqdm(range(len(users))):\n",
        "    user_chb = mapping_index_chb[userid]\n",
        "    user_rec = get_recom(userid)\n",
        "    for rec in user_rec:\n",
        "        all_rec.append([user_chb, rec])\n",
        "\n"
      ],
      "metadata": {
        "id": "4MMjJDciIaW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сохранение итогового прогноза\n",
        "solution = pd.DataFrame(all_rec, columns=[\"chb\", \"sys_numb\"])\n",
        "solution.to_csv(\"solution.csv\", index=False, sep=';')"
      ],
      "metadata": {
        "id": "YsgdxyczIaTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}